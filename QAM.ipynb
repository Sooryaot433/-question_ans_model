{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ================================\n",
        "# Step 1: Dataset definition\n",
        "# ================================\n",
        "class SquadDataset(Dataset):\n",
        "    def __init__(self, filename, max_vocab_size=10000, max_passage_length=300, max_question_length=30):\n",
        "        self.max_passage_length = max_passage_length\n",
        "        self.max_question_length = max_question_length\n",
        "        self.data = []\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "        # Load SQuAD JSON\n",
        "        with open(filename, 'r') as f:\n",
        "            squad = json.load(f)\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.build_vocab(squad, max_vocab_size)\n",
        "\n",
        "        # Process each paragraph and question-answer pair\n",
        "        for article in squad['data']:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                context = paragraph['context']\n",
        "                context_tokens = self.tokenize(context)\n",
        "                context_ids = self.encode(context_tokens, self.max_passage_length)\n",
        "\n",
        "                for qa in paragraph['qas']:\n",
        "                    question = qa['question']\n",
        "                    question_tokens = self.tokenize(question)\n",
        "                    question_ids = self.encode(question_tokens, self.max_question_length)\n",
        "\n",
        "                    answer = qa['answers'][0]\n",
        "                    answer_text = answer['text']\n",
        "                    answer_start = answer['answer_start']\n",
        "\n",
        "                    # Approximate token alignment\n",
        "                    start_idx, end_idx = self.find_answer_span(context, context_tokens, answer_text, answer_start)\n",
        "                    if start_idx == -1 or end_idx == -1:\n",
        "                        continue\n",
        "                    if start_idx >= self.max_passage_length or end_idx >= self.max_passage_length:\n",
        "                        continue\n",
        "\n",
        "                    self.data.append({\n",
        "                        'context': context_ids,\n",
        "                        'question': question_ids,\n",
        "                        'start': start_idx,\n",
        "                        'end': end_idx\n",
        "                    })\n",
        "\n",
        "    def build_vocab(self, squad, max_vocab_size):\n",
        "        word_counts = Counter()\n",
        "        for article in squad['data']:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                word_counts.update(self.tokenize(paragraph['context']))\n",
        "                for qa in paragraph['qas']:\n",
        "                    word_counts.update(self.tokenize(qa['question']))\n",
        "        # Top words\n",
        "        most_common = word_counts.most_common(max_vocab_size - 2)\n",
        "        vocab = ['<pad>', '<unk>'] + [w for w, _ in most_common]\n",
        "        self.word2idx = {w: idx for idx, w in enumerate(vocab)}\n",
        "        self.idx2word = {idx: w for idx, w in enumerate(vocab)}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(f'[{string.punctuation}]', '', text)\n",
        "        return text.split()\n",
        "\n",
        "    def encode(self, tokens, max_length):\n",
        "        ids = [self.word2idx.get(t, self.word2idx['<unk>']) for t in tokens]\n",
        "        if len(ids) > max_length:\n",
        "            ids = ids[:max_length]\n",
        "        else:\n",
        "            ids += [self.word2idx['<pad>']] * (max_length - len(ids))\n",
        "        return ids\n",
        "\n",
        "    def detokenize(self, tokens):\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def find_answer_span(self, context, tokens, answer_text, char_start):\n",
        "        # Naive span finding\n",
        "        text_lower = context.lower()\n",
        "        answer_lower = answer_text.lower()\n",
        "        char_end = char_start + len(answer_text)\n",
        "        answer_in_context = text_lower[char_start:char_end]\n",
        "\n",
        "        # Reconstruct text from tokens to approximate token-level alignment\n",
        "        for i in range(len(tokens)):\n",
        "            for j in range(i, len(tokens)):\n",
        "                span_text = \" \".join(tokens[i:j+1])\n",
        "                if span_text == answer_lower:\n",
        "                    return i, j\n",
        "        return -1, -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            'context': torch.tensor(item['context'], dtype=torch.long),\n",
        "            'question': torch.tensor(item['question'], dtype=torch.long),\n",
        "            'start': torch.tensor(item['start'], dtype=torch.long),\n",
        "            'end': torch.tensor(item['end'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ================================\n",
        "# Step 2: Model definition\n",
        "# ================================\n",
        "class QAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(QAModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.passage_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.question_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.linear_start = nn.Linear(hidden_dim * 4, 1)\n",
        "        self.linear_end = nn.Linear(hidden_dim * 4, 1)\n",
        "\n",
        "    def forward(self, context, question):\n",
        "        context_embed = self.embedding(context)\n",
        "        question_embed = self.embedding(question)\n",
        "\n",
        "        passage_output, _ = self.passage_lstm(context_embed)\n",
        "        question_output, _ = self.question_lstm(question_embed)\n",
        "\n",
        "        question_repr = torch.mean(question_output, dim=1)\n",
        "        question_repr = question_repr.unsqueeze(1).repeat(1, passage_output.size(1), 1)\n",
        "\n",
        "        combined = torch.cat([passage_output, question_repr], dim=-1)\n",
        "\n",
        "        start_logits = self.linear_start(combined).squeeze(-1)\n",
        "        end_logits = self.linear_end(combined).squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits\n",
        "\n",
        "# ================================\n",
        "# Helper functions\n",
        "# ================================\n",
        "def create_mask(tensor, pad_idx=0):\n",
        "    return (tensor != pad_idx).float()\n",
        "\n",
        "def masked_logits(logits, mask):\n",
        "    return logits * mask - 1e9 * (1 - mask)\n",
        "\n",
        "# ================================\n",
        "# Step 3: Training loop\n",
        "# ================================\n",
        "# Hyperparameters\n",
        "max_vocab_size = 10000\n",
        "max_passage_length = 300\n",
        "max_question_length = 30\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = SquadDataset('train-v1.1.json', max_vocab_size, max_passage_length, max_question_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "vocab_size = len(train_dataset.word2idx)\n",
        "model = QAModel(vocab_size).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        context = batch['context'].to(device)\n",
        "        question = batch['question'].to(device)\n",
        "        start_pos = batch['start'].to(device)\n",
        "        end_pos = batch['end'].to(device)\n",
        "\n",
        "        mask = create_mask(context).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        start_logits, end_logits = model(context, question)\n",
        "\n",
        "        start_logits = masked_logits(start_logits, mask)\n",
        "        end_logits = masked_logits(end_logits, mask)\n",
        "\n",
        "        loss_start = criterion(start_logits, start_pos)\n",
        "        loss_end = criterion(end_logits, end_pos)\n",
        "        loss = loss_start + loss_end\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ================================\n",
        "# Step 4: Inference\n",
        "# ================================\n",
        "def predict(context_text, question_text, model, dataset, max_passage_length=300, max_question_length=30):\n",
        "    model.eval()\n",
        "    context_tokens = dataset.tokenize(context_text)\n",
        "    context_ids = dataset.encode(context_tokens, max_passage_length)\n",
        "\n",
        "    question_tokens = dataset.tokenize(question_text)\n",
        "    question_ids = dataset.encode(question_tokens, max_question_length)\n",
        "\n",
        "    context_tensor = torch.tensor([context_ids], dtype=torch.long).to(device)\n",
        "    question_tensor = torch.tensor([question_ids], dtype=torch.long).to(device)\n",
        "    mask = create_mask(context_tensor).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_logits, end_logits = model(context_tensor, question_tensor)\n",
        "        start_logits = masked_logits(start_logits, mask)\n",
        "        end_logits = masked_logits(end_logits, mask)\n",
        "        start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "        end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    if start_idx > end_idx or end_idx >= len(context_tokens):\n",
        "        return \"I couldn't find a valid answer.\"\n",
        "\n",
        "    answer_tokens = context_tokens[start_idx:end_idx + 1]\n",
        "    predicted_answer = dataset.detokenize(answer_tokens)\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "# ================================\n",
        "# Example usage\n",
        "# ================================\n",
        "context_example = \"The Transformers library was created by Hugging Face in 2018 to provide state-of-the-art NLP models.\"\n",
        "question_example = \"Who created the Transformers library?\"\n",
        "\n",
        "answer = predict(context_example, question_example, model, train_dataset)\n",
        "print(\"\\n=== Prediction Example ===\")\n",
        "print(f\"Context: {context_example}\")\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Predicted Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe8HeZgsE758",
        "outputId": "523e92e8-9631-4d9e-8d68-b290bcda735f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}